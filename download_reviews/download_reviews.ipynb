{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Reviews\n",
    "\n",
    "This script downloads game metadata and review data for use in later summarisation.\n",
    "\n",
    "Sections:\n",
    "* [Download Game Details](#game_details)\n",
    "* [Download Game Reviews](#game_reviews)\n",
    "\n",
    "\n",
    "Code based on https://github.com/zhenzuo2/IS_590_Final/tree/master/Gather_Data\n",
    "\n",
    "Files generated:\n",
    "* game_details.csv - metadata for each game pulled from the Steam description. E.g. `name`, `app_id`, `genres`, `release_date`.\n",
    "* completed_pages.pkl - pickled dictionary tracking the number of pages of reviews that have been downloaded for each game. Used to resume download from the previous position.\n",
    "* [app_id].csv - csv file storing reviews.\n",
    "\n",
    "Known Limitations:\n",
    "* Resuming downloads is based on the assumption that minimal new reviews have been posted since the start of the download. While there is logic for handling duplicates, the script will not automatically pull reviews that have been posted since the last download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_folder_directory = Path('../review_data_2')\n",
    "\n",
    "# If true, write game details file to disk. This will overwrite previous files.\n",
    "WRITE_GAME_DETAILS = True\n",
    "\n",
    "# IDs for games wanted to download.\n",
    "app_id_list = [\n",
    "    570, # Dota 2\n",
    "    578080, # PUBG\n",
    "    730, # CS:GO\n",
    "    230410, # Warframe\n",
    "    359550, # Rainbow Six: Siege\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Game Details <a name=\"game_details\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download game details based on app id\n",
    "def get_details(app_id: int, max_attempts: int = 10) -> dict:\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        response = requests.get(\"http://store.steampowered.com/api/appdetails?appids=\" + str(app_id))\n",
    "        if response.status_code != 429:\n",
    "            return response.json()\n",
    "        # If rate limited, wait and try again\n",
    "        time.sleep((2 ** attempts) + random.random())\n",
    "        attempts = attempts + 1\n",
    "    print(f\"Unable to retrieve product details for app_id = {app_id}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download details for all games in the list\n",
    "details = {}\n",
    "for app_id in tqdm(app_id_list):\n",
    "    d = get_details(app_id)\n",
    "    if d is not None:\n",
    "        details.update(d)\n",
    "        \n",
    "# Delete the \"success\" field, and create a datafrane of game data.\n",
    "for key in details.keys():\n",
    "    if details[key]['success'] :\n",
    "        details[key] = details[key]['data']\n",
    "    else:\n",
    "        details[key] = None\n",
    "        \n",
    "game_details = pd.DataFrame.from_dict(details, orient='index')\n",
    "game_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to disk.\n",
    "if WRITE_GAME_DETAILS:\n",
    "    reviews_folder_directory.mkdir(parents=True, exist_ok=True)\n",
    "    game_details_directory = f'{reviews_folder_directory}/game_details.csv'\n",
    "    game_details.to_csv(game_details_directory, sep = '|', escapechar = '@', index = True)\n",
    "    print(f\"Wrote game details to {game_details_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate map from game id to game name\n",
    "game_name_map = dict(zip(game_details.steam_appid, game_details.name))\n",
    "pprint(game_name_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Game Reviews <a name=\"game_reviews\"/>\n",
    "This section downloads reviews from Steam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(app_id: int, page: int, max_attempts: int = 10) -> dict:\n",
    "    \"\"\"Get a page of reviews for a given game.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    app_id : int\n",
    "        app_id to get reviews of\n",
    "    page : int\n",
    "        page number to get reviews of. Page 0 is reviews 0 - 99, page 1 is reviews 100-199, etc.\n",
    "    max_attempts: int, optional\n",
    "        maximum number of retries if steam rejects the API call. \n",
    "        Total time to timeout = (max_attempts * (max_attempts-1)) seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list(dict())\n",
    "        list of reviews, each review being a dictionary\n",
    "        \n",
    "    Documentation: https://partner.steamgames.com/doc/store/getreviews\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    # Each page is 100 reviews, so we offset by 100 each time\n",
    "    offset = page * 100\n",
    "    while attempts < max_attempts:\n",
    "        # Get recent reviews, in english, with 100 per page.\n",
    "        response = requests.get(\"http://store.steampowered.com/appreviews/\"+str(app_id)+ \n",
    "                                \"?json=1&filter=recent&language=english&num_per_page=100&start_offset=\" + str(offset))\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"reviews\"]\n",
    "        # If rate limited, wait and try again\n",
    "        time.sleep((2 * attempts))\n",
    "        attempts = attempts + 1\n",
    "    print(f\"Unable to retrieve reviews for {game_name_map[app_id]} (app_id = {app_id})\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_author(review_json: dict) -> dict:\n",
    "    \"\"\"In-place function to flatten author data into the main attributes.\n",
    "\n",
    "    Keys of the review_json[\"author\"] are added to review_json with \"author__\" prepended.\n",
    "    \"\"\"\n",
    "    author_data = review_json.pop('author')\n",
    "    author_data = {'author__' + key : value for key, value in author_data.items()}\n",
    "    review_json.update(author_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_download_limit = 50 # Number of pages to download for each game before breaking.\n",
    "\n",
    "# Download reviews for every game in list and add to a pandas df\n",
    "\n",
    "# Try and resume from the last page downloaded - otherwise, start from 0.\n",
    "try:\n",
    "    with open(f\"{reviews_folder_directory}/completed_pages.pkl\",\"rb\") as rf:\n",
    "        completed_pages = pickle.load(rf)\n",
    "    print(\"Loaded completed pages file. Starting completition:\\n\", {game_name_map[k]: v for k,v in completed_pages.items()})\n",
    "except FileNotFoundError:\n",
    "    print(\"No completed pages found, starting from zero.\")\n",
    "    completed_pages = {k : 0 for k in app_id_list}\n",
    "\n",
    "# For each game, download reviews\n",
    "for app_id in app_id_list:\n",
    "    # Remove one to check the page that failed previously\n",
    "    completed_pages[app_id] = max(0, completed_pages[app_id] - 1)\n",
    "    \n",
    "    # Try to add to the existing review dataframe, otherwise create a new one.\n",
    "    try:\n",
    "        game_reviews = pd.read_csv(f'{reviews_folder_directory}/{app_id}.csv', sep = '|', )\n",
    "    except:\n",
    "        game_reviews = pd.DataFrame(columns=['recommendationid', 'author__steamid', 'author__num_games_owned',\n",
    "                                             'author__num_reviews', 'author__playtime_forever', \n",
    "                                             'author__playtime_last_two_weeks', 'author__last_played',\n",
    "                                             'language', 'review', 'timestamp_created', 'timestamp_updated', \n",
    "                                             'voted_up', # If the review was voted positive\n",
    "                                             'votes_up', 'votes_funny', 'comment_count',\n",
    "                                             'steam_purchase', 'received_for_free', \n",
    "                                             'written_during_early_access'])\n",
    "        \n",
    "    game_reviews = game_reviews.astype({\"voted_up\":bool,\"steam_purchase\":bool, \"received_for_free\":bool, \"written_during_early_access\":bool})\n",
    "\n",
    "    # Loop until we no longer get a result\n",
    "    with tqdm(desc=f\"Downloading reviews for {game_name_map[app_id]} (app_id = {app_id}) \") as bar:\n",
    "        try:\n",
    "            for i in range(page_download_limit):\n",
    "                current_reviews = get_reviews(app_id, completed_pages[app_id])\n",
    "                if current_reviews == []:\n",
    "                    break\n",
    "                for review in current_reviews:\n",
    "                    flatten_author(review)\n",
    "                # Only keep the review information we save as columns\n",
    "                current_reviews = [{k : review[k] for k in game_reviews.columns} for review in current_reviews]\n",
    "                game_reviews = pd.concat((game_reviews, pd.DataFrame(current_reviews)))\n",
    "                # Incremenet offset\n",
    "                completed_pages[app_id] += 1\n",
    "                bar.update(1)\n",
    "                \n",
    "        # If we keyboard interupt, stop getting the current review and instead save our data.\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "    \n",
    "    # Set reccommendationid to neumeric (for dropping duplicates)\n",
    "    length_with_dupes = game_reviews.shape[0]\n",
    "    game_reviews['recommendationid'] = game_reviews['recommendationid'].apply(pd.to_numeric)\n",
    "    #game_reviews.drop_duplicates('recommendationid', keep = 'last', inplace=True)\n",
    "    \n",
    "    print(f\"Removed {length_with_dupes - game_reviews.shape[0]} duplicate elements.\")\n",
    "    \n",
    "    \n",
    "    # Save results \n",
    "    game_reviews.to_csv(f'{reviews_folder_directory}/{app_id}.csv', sep = '|', escapechar = '@', index = False)\n",
    "       \n",
    "print()\n",
    "print(\"Download finished/interrupted. Completed pages:\")\n",
    "pprint({game_name_map[k]: v for k,v in completed_pages.items()})\n",
    "\n",
    "with open(f\"{reviews_folder_directory}/completed_pages.pkl\",\"wb\") as wf:\n",
    "    pickle.dump(completed_pages, wf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
